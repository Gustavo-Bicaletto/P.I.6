# ðŸ¤– Treinamento de Modelo ML para ClassificaÃ§Ã£o de CurrÃ­culos

Este guia explica como treinar um modelo de Machine Learning para classificar currÃ­culos automaticamente.

## ðŸ“‹ PrÃ©-requisitos

1. Python 3.8+
2. MongoDB com dados processados (coleÃ§Ã£o `dados_processados`)
3. GPU NVIDIA (opcional, mas recomendado para treino mais rÃ¡pido)

## ðŸš€ Passo a Passo

### 1. Instalar DependÃªncias

```bash
# Instalar dependÃªncias de ML
pip install -r requirements-ml.txt

# OU se tiver GPU NVIDIA (muito mais rÃ¡pido):
pip install torch --index-url https://download.pytorch.org/whl/cu118
pip install transformers datasets scikit-learn accelerate tqdm
```

### 2. Preparar Dados de Treinamento

Este passo extrai dados do MongoDB e cria datasets balanceados:

```bash
# Preparar dados (balanceado, mÃ­nimo 2 anos para "experienced")
python -m app.ml.prepare_training_data --min-years 2.0

# OpÃ§Ãµes avanÃ§adas:
python -m app.ml.prepare_training_data \
  --min-years 2.0 \
  --max-per-class 1000 \
  --test-size 0.2 \
  --val-size 0.1 \
  --output-dir data/training
```

**SaÃ­da esperada:**
- `data/training/train.json` - Dados de treino
- `data/training/validation.json` - Dados de validaÃ§Ã£o
- `data/training/test.json` - Dados de teste
- `data/training/metadata.json` - Metadados do dataset

### 3. Treinar o Modelo

```bash
# Treino bÃ¡sico com DistilBERT (modelo menor e rÃ¡pido)
python -m app.ml.train_model --model distilbert-base-uncased --epochs 3

# Com GPU (recomendado):
python -m app.ml.train_model \
  --model distilbert-base-uncased \
  --epochs 3 \
  --batch-size 16 \
  --lr 2e-5

# Sem GPU (mais lento):
python -m app.ml.train_model \
  --model distilbert-base-uncased \
  --epochs 3 \
  --batch-size 8 \
  --no-gpu
```

**Modelos disponÃ­veis:**

| Modelo | Tamanho | Velocidade | Qualidade | RecomendaÃ§Ã£o |
|--------|---------|------------|-----------|--------------|
| `distilbert-base-uncased` | 66M | âš¡âš¡âš¡ | â­â­â­ | **InÃ­cio** |
| `bert-base-uncased` | 110M | âš¡âš¡ | â­â­â­â­ | ProduÃ§Ã£o |
| `roberta-base` | 125M | âš¡âš¡ | â­â­â­â­ | Melhor qualidade |

**SaÃ­da esperada:**
- `models/resume_classifier/` - Modelo treinado
- `models/resume_classifier/training_results.json` - MÃ©tricas de avaliaÃ§Ã£o

### 4. Testar o Modelo

```bash
# Testar com documentos do MongoDB
python -m app.ml.predict --model models/resume_classifier --from-db --limit 10

# Testar documento especÃ­fico
python -m app.ml.predict --model models/resume_classifier --doc-id "673abc123def456789012345"

# Testar com texto direto
python -m app.ml.predict --model models/resume_classifier --text "Senior Software Engineer with 10 years..."

# Testar com arquivo
python -m app.ml.predict --model models/resume_classifier --file curriculo.txt
```

## ðŸ“Š MÃ©tricas de AvaliaÃ§Ã£o

ApÃ³s o treinamento, vocÃª verÃ¡ mÃ©tricas como:

```
ðŸŽ¯ Resultados no Teste:
   Accuracy:  0.9250
   F1 Score:  0.9180
   Precision: 0.9300
   Recall:    0.9060
```

**O que significam:**
- **Accuracy**: % de acertos totais
- **F1 Score**: MÃ©dia harmÃ´nica de precisÃ£o e recall (mÃ©trica principal)
- **Precision**: Dos que previu como "experienced", quantos realmente sÃ£o
- **Recall**: Dos que sÃ£o "experienced", quantos o modelo encontrou

## âš™ï¸ ConfiguraÃ§Ãµes AvanÃ§adas

### Ajustar HiperparÃ¢metros

```bash
python -m app.ml.train_model \
  --model distilbert-base-uncased \
  --epochs 5 \
  --batch-size 32 \
  --lr 3e-5 \
  --max-length 768
```

### Usar GPU Mais Eficientemente

Se vocÃª tem GPU NVIDIA, o treinamento usarÃ¡ automaticamente:
- **Mixed Precision (FP16)**: 2x mais rÃ¡pido, usa menos memÃ³ria
- **Gradient Accumulation**: Para simular batch sizes maiores

### Aumentar Dataset

```bash
# Usar mais amostras por classe
python -m app.ml.prepare_training_data --max-per-class 2000

# NÃ£o balancear (usar todos os dados)
python -m app.ml.prepare_training_data --no-balance
```

## ðŸ› Troubleshooting

### CUDA Out of Memory
```bash
# Reduzir batch size
python -m app.ml.train_model --batch-size 8
```

### Modelo nÃ£o converge
```bash
# Aumentar Ã©pocas ou learning rate
python -m app.ml.train_model --epochs 5 --lr 3e-5
```

### Desbalanceamento de classes
```bash
# Verificar metadata.json apÃ³s preparar dados
cat data/training/metadata.json
```

## ðŸ“¦ Estrutura de Arquivos

```
data/
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train.json           # Dados de treino
â”‚   â”œâ”€â”€ validation.json      # Dados de validaÃ§Ã£o
â”‚   â”œâ”€â”€ test.json           # Dados de teste
â”‚   â””â”€â”€ metadata.json       # Metadados

models/
â””â”€â”€ resume_classifier/
    â”œâ”€â”€ config.json         # ConfiguraÃ§Ã£o do modelo
    â”œâ”€â”€ pytorch_model.bin   # Pesos do modelo
    â”œâ”€â”€ tokenizer_config.json
    â”œâ”€â”€ vocab.txt
    â””â”€â”€ training_results.json  # Resultados do treino
```

## ðŸŽ¯ PrÃ³ximos Passos

1. âœ… Treinar modelo inicial
2. âœ… Avaliar mÃ©tricas
3. ðŸ”„ Ajustar hiperparÃ¢metros se necessÃ¡rio
4. ðŸš€ Integrar modelo no sistema de scoring
5. ðŸ“ˆ Re-treinar periodicamente com novos dados

## ðŸ’¡ Dicas

- **Comece pequeno**: Use `distilbert-base-uncased` primeiro
- **GPU Ã© essencial**: Treino sem GPU pode levar horas
- **Monitore mÃ©tricas**: F1 Score > 0.85 Ã© bom para comeÃ§ar
- **Balance os dados**: Classes desbalanceadas prejudicam o modelo
- **Re-treine regularmente**: Com novos currÃ­culos processados

## ðŸ“š DocumentaÃ§Ã£o

- [Transformers](https://huggingface.co/docs/transformers)
- [PyTorch](https://pytorch.org/docs/stable/index.html)
- [Datasets](https://huggingface.co/docs/datasets)
